{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Programming Exercise 6: Spam Classification with SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import package(s)\n",
    "import numpy as np\n",
    "import scipy.io as sio \n",
    "from sklearn import svm \n",
    "import regex as re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Spam Classification\n",
    "In the next half of the exercise, support vector machines (SVM) are used to build a spam classiﬁer.\n",
    "\n",
    "A classifier is trained to classify whether a given email, $x$, is spam ($y = 1$) or non-spam ($y = 0$). Each email needs to be converted into a feature vector $x \\in \\mathbb{R}^n$. The following parts of the exercise is a walk through of how such a feature vector can be constructed from an email. \n",
    "\n",
    "The dataset included for this exercise is based on a a subset of the [SpamAssassin Public Corpus](http://spamassassin.apache.org/old/publiccorpus/). For the purpose of this exercise, only the body of the email is used (excluding the email headers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocessing Emails\n",
    "Before starting on a machine learning task, it is usually insightful to take a look at examples from the dataset.\n",
    "\n",
    "Image Source: ex6.pdf from Andrew Ng's Coursera course in Machine Learning\n",
    "![sampemail.png](sampemail.png)\n",
    "\n",
    "Figure 8 shows a sample email that contains a URL, an email address (at the end), numbers, and dollar amounts. While many emails would contain similar types of entities (e.g., numbers, other URLs, or other email addresses), the speciﬁc entities (e.g., the speciﬁc URL or speciﬁc dollar amount) will be diﬀerent in almost every email. Therefore, one method often employed in processing emails is to “normalize” these values, so that all URLs are treated the same, all numbers are treated the same, etc. For example, we each URL in the email could be replace with the unique string \"httpaddr\" to indicate that a URL was present.\n",
    "\n",
    "This has the eﬀect of letting the spam classiﬁer make a classiﬁcation decision based on whether any URL was present, rather than whether a speciﬁc URL was present. This typically improves the performance of a spam classiﬁer, since spammers often randomize the URLs, and thus the odds of seeing any particular URL again in a new piece of spam is very small.\n",
    "\n",
    "In the function processEmail, the following email preprocessing and normalization steps are implemented:\n",
    "* **Lower-casing**: The entire email is converted into lower case, so that captialization is ignored (e.g., IndIcaTE is treated the same as Indicate).\n",
    "* **Stripping HTML**: All HTML tags are removed from the emails. Many emails often come with HTML formatting; we remove all the HTML tags, so that only the content remains.\n",
    "* **Normalizing URLs**: All URLs are replaced with the text \"httpaddr\".\n",
    "* **Normalizing Email Addresses**: All email addresses are replaced with the text \"emailaddr\".\n",
    "* **Normalizing Numbers**: All numbers are replaced with the text \"number\".\n",
    "* **Normalizing Dollars**: All dollar signs ($\\$$) are replaced with the text \"dollar\".\n",
    "* **Word Stemming**: Words are reduced to their stemmed form. For example, \"discount\", \"discounts\", \"discounted\" and \"discounting\" are all replaced with \"discount\". Sometimes, the Stemmer actually strips oﬀ additional characters from the end, so \"include\", \"includes\", \"included\", and \"including\" are all replaced with \"includ\".\n",
    "* **Removal of non-words**: Non-words and punctuation have been removed. All white spaces (tabs, newlines, spaces) have all been trimmed to a single space character.\n",
    "\n",
    "The result of these preprocessing steps is shown in Figure 9. While preprocessing has left word fragments and non-words, this form turns out to be much easier to work with for performing feature extraction.\n",
    "\n",
    "Image Source: ex6.pdf from Andrew Ng's Coursera course in Machine Learning\n",
    "![preproemail.png](preproemail.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Vocabulary List\n",
    "After preprocessing the emails, there is a list of words (e.g., Figure 9) for each email. The next step is to choose which words to use in the classiﬁer and which to leave out.\n",
    "\n",
    "For this exercise, only the most frequently occuring words are chosen as the set of words considered (the vocabulary list). Since words that occur rarely in the training set are only in a few emails, they might cause the model to overﬁt the training set. The complete vocabulary list is in the ﬁle vocab.txt and also shown in Figure 10. The vocabulary list was selected by choosing all words which occur at least a 100 times in the spam corpus, resulting in a list of 1899 words. In practice, a vocabulary list with about 10,000 to 50,000 words is often used. \n",
    "\n",
    "Given the vocabulary list, each word can now be mapped in the preprocessed emails (e.g., Figure 9) into a list of word indices that contains the index of the word in the vocabulary list. Figure 11 shows the mapping for the sample email. Speciﬁcally, in the sample email, the word \"anyone\" was ﬁrst normalized to \"anyon\" and then mapped onto the index 86 in the vocabulary list.\n",
    "\n",
    "Image Source: ex6.pdf from Andrew Ng's Coursera course in Machine Learning\n",
    "![vocabind.png](vocabind.png)\n",
    "\n",
    "Complete the code in processEmail to perform this mapping. In the code, a string str is a single word from the processed email. Look up the word in the vocabulary list vocabList and ﬁnd if the word exists in the vocabulary list. If the word exists, add the index of the word into the word indices variable. If the word does not exist, and is therefore not in the vocabulary, skip the word.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    By using a stemming software that basically looks at the first few letters of a word, more of less, it can help, but it can also hurt. For example, the software may mistake the words universe and university as being the same thing because these two words start off with the same letters.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing sample email (emailSample1.txt)\n",
      "\n",
      "\n",
      "==== Processed Email ====\n",
      "\n",
      "\n",
      "Word Indices: \n",
      "\n",
      "[86, 916, 794, 1077, 883, 370, 1699, 790, 1822, 1831, 883, 431, 1171, 794, 1002, 1895, 592, 1676, 238, 162, 89, 688, 945, 1663, 1120, 1062, 1699, 375, 1162, 479, 1893, 1510, 799, 1182, 1237, 810, 1895, 1440, 1547, 181, 1699, 1758, 1896, 688, 1676, 992, 961, 1477, 71, 530, 1699, 531]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#define function for exercise(s)\n",
    "def getVocabList():\n",
    "#GETVOCABLIST reads the fixed vocabulary list in vocab.txt and returns a\n",
    "#cell array of the words\n",
    "#   vocabList = GETVOCABLIST() reads the fixed vocabulary list in vocab.txt \n",
    "#   and returns a cell array of the words in vocabList.\n",
    "\n",
    "    #Read the fixed vocabulary list\n",
    "    fid = open('vocab.txt')\n",
    "\n",
    "    #Store all dictionary words in cell array vocab{}\n",
    "    #n = 1899  # Total number of words in the dictionary\n",
    "    \n",
    "    #For ease of implementation, we use a struct to map the strings => integers\n",
    "    #In practice, you'll want to use some form of hashmap\n",
    "    vocabList = []\n",
    "    for i in fid:\n",
    "        index, word = i.split()\n",
    "        vocabList.append(word)\n",
    "    \n",
    "    fid.close()\n",
    "    return vocabList\n",
    "\n",
    "def processEmail(email_contents):\n",
    "#PROCESSEMAIL preprocesses a the body of an email and\n",
    "#returns a list of word_indices \n",
    "#   word_indices = PROCESSEMAIL(email_contents) preprocesses \n",
    "#   the body of an email and returns a list of indices of the \n",
    "#   words contained in the email. \n",
    "\n",
    "    #Load Vocabulary\n",
    "    vocabList = getVocabList()\n",
    "\n",
    "    #Init return value\n",
    "    word_indices = []\n",
    "\n",
    "    # Preprocess Email \n",
    "    \n",
    "    #Find the Headers ( \\n\\n and remove )\n",
    "    #Uncomment the following lines if you are working with raw emails with the\n",
    "    #full headers\n",
    "\n",
    "    #hdrstart = strfind(email_contents, ([char(10) char(10)]));\n",
    "    #email_contents = email_contents(hdrstart(1):end);\n",
    "    \n",
    "    #Lower case\n",
    "    email_contents = [line.lower() for line in email_contents]\n",
    "    \n",
    "    #Strip all HTML\n",
    "    #Looks for any expression that starts with < and ends with > and replace\n",
    "    #and does not have any < or > in the tag it with a space\n",
    "    email_contents = [re.sub('^[<]|[<.*>]|[>]$', '', line) for line in email_contents]\n",
    "    \n",
    "    #Handle Numbers\n",
    "    #Look for one or more characters between 0-9\n",
    "    email_contents = [re.sub('\\d+', 'number', line) for line in email_contents]\n",
    "    \n",
    "    #Handle URLS\n",
    "    #Look for strings starting with http:// or https://\n",
    "    email_contents = [re.sub('(http|https)://+[^\\s]+.*?', 'httpaddr', line) for line in email_contents]\n",
    "    \n",
    "    #Handle Email Addresses\n",
    "    #Look for strings with @ in the middle\n",
    "    email_contents = [re.sub('[^\\s]+@[^\\s]+', 'emailaddr', line) for line in email_contents]\n",
    "\n",
    "    #Handle $ sign\n",
    "    email_contents = [re.sub('[$]+', 'dollar', line) for line in email_contents]\n",
    "    \n",
    "    #========================== Tokenize Email ===========================\n",
    "\n",
    "    #Output the email to screen as well\n",
    "    print('\\n==== Processed Email ====\\n\\n')\n",
    "\n",
    "    #Process file\n",
    "    \n",
    "    #Get rid of any punctuation and leading and ending spaces \n",
    "    email_contents = [re.sub(r'[^\\w\\s]','',line) for line in email_contents]\n",
    "    email_contents = [line.strip() for line in email_contents]\n",
    "    \n",
    "    #Remove any non alphanumeric characters\n",
    "    #email_contents = [re.sub(r'\\W+','',line) for line in email_contents]\n",
    "    \n",
    "    #Tokenize (split into an array of words)\n",
    "    email_contents = word_tokenize(' '.join(email_contents))\n",
    "    \n",
    "    #Stem the word \n",
    "    #(the porterStemmer sometimes has issues, so we use a try catch block)\n",
    "    ps = PorterStemmer()\n",
    "    stem_word = []\n",
    "    for w in email_contents:\n",
    "        stem_word.append(ps.stem(w))\n",
    "    \n",
    "    #Look up the word in the dictionary and add to word_indices if found\n",
    "    #add one since Python starts its index at zero instead of one\n",
    "    for i in range(len(stem_word)):\n",
    "        if stem_word[i] in vocabList:\n",
    "            word_indices.append((vocabList.index(stem_word[i]))+1)\n",
    "        \n",
    "    return word_indices\n",
    "    \n",
    "# ==================== Part 1: Email Preprocessing ====================\n",
    "#  To use an SVM to classify emails into Spam v.s. Non-Spam, you first need\n",
    "#  to convert each email into a vector of features. In this part, you will\n",
    "#  implement the preprocessing steps for each email. You should\n",
    "#  complete the code in processEmail.m to produce a word indices vector\n",
    "#  for a given email.\n",
    "\n",
    "print('\\nPreprocessing sample email (emailSample1.txt)\\n')\n",
    "\n",
    "#Extract Features\n",
    "file_contents = open('emailSample1.txt')\n",
    "word_indices  = processEmail(file_contents)\n",
    "\n",
    "#Print Stats\n",
    "print('Word Indices: \\n')\n",
    "print(word_indices)\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Implementation Note:</b> \n",
    "    To find the index of a matched string in vocabList use vocabList.index(str).\n",
    "    Speciﬁcally, to get the word at index $i$, you can use vocabList[i].\n",
    "    Use len(vocabList) to get the number of words in the vocabulary.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extracting Features from Emails\n",
    "Implement the feature extraction that converts each email into a vector in $\\mathbb{R}^n$. For this exercise, use $n = \\#$ words in vocabulary list. Specifically, the feature $x_i \\in \\{0,1\\}$ for an email corresponds to whether the $i$-th word in the dictionary occurs in the email. That is, $x_i = 1$ if the $i$-th word is in the email and $x_i = 0$ if the $i$-th word is not present in the email. \n",
    "\n",
    "Thus, for a typical email, this feature would look like:\n",
    "$x = \\begin{bmatrix}\n",
    "       0 \\\\\n",
    "       \\vdots \\\\\n",
    "       1 \\\\\n",
    "       0 \\\\\n",
    "       \\vdots \\\\\n",
    "       1 \\\\ \n",
    "       0 \\\\\n",
    "       \\vdots \\\\\n",
    "       0\n",
    "       \\end{bmatrix} \\in \\mathbb{R}^n.$\n",
    "       \n",
    "Implement the function emailFeatures on the email sample. The feature vector should had a length $1899$ and $45$ non-zero entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting features from sample email (emailSample1.txt)\n",
      "\n",
      "Length of feature vector: 1899\n",
      "Number of non-zero entries: 44\n"
     ]
    }
   ],
   "source": [
    "#define function for exercise(s)\n",
    "def emailFeatures(word_indices):\n",
    "#EMAILFEATURES takes in a word_indices vector and produces a feature vector\n",
    "#from the word indices\n",
    "#   x = EMAILFEATURES(word_indices) takes in a word_indices vector and \n",
    "#   produces a feature vector from the word indices.     \n",
    "    \n",
    "    #Total number of words in the dictionary\n",
    "    vocabList = getVocabList()\n",
    "    n = len(vocabList)\n",
    "\n",
    "    #You need to return the following variables correctly.\n",
    "    feat_vec = np.zeros((n, 1))\n",
    "    \n",
    "    for index in range(n):\n",
    "        if index in word_indices:\n",
    "            feat_vec[index-1]=1\n",
    "    return feat_vec\n",
    "\n",
    "# ==================== Part 2: Feature Extraction ====================\n",
    "#  Now, you will convert each email into a vector of features in R^n. \n",
    "#  You should complete the code in emailFeatures.m to produce a feature\n",
    "#  vector for a given email.\n",
    "\n",
    "print('\\nExtracting features from sample email (emailSample1.txt)\\n')\n",
    "\n",
    "#Extract Features\n",
    "features = emailFeatures(word_indices)\n",
    "\n",
    "#Print Stats\n",
    "print('Length of feature vector:', len(features))\n",
    "print('Number of non-zero entries:', np.sum(features > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training SVM for Spam Classiﬁcation\n",
    "After completing the feature extraction functions, the next step to train a SVM classfier with a preprocessed training dataset. spamTrain.mat contains $4000$ training examples of spam and non-spam email, while spamTest.mat contains $1000$ test examples. Each original email was processed using the processEmail and emailFeatures functions and converted into a vector $x^{(i)} \\in \\mathbb{R}^{1899}$.\n",
    "\n",
    "After loading the dataset, the SVM is trained to classify between spam $(y = 1)$ and non-spam $(y = 0)$ emails. Once the training completes, the classiﬁer should get a training accuracy of about $99.8\\%$ and a test accuracy of about $98.5\\%.$\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> \n",
    "    It is very important to get error results as a single, numerical value otherwise it is difficult to assess your algorithm’s performance. For example if we use stemming, which is the process of treating the same word with different forms (fail/failing/failed) as one word (fail), and get a $3\\%$ error rate instead of $5\\%$, then we should definitely add it to our model. However, if we try to distinguish between upper case and lower case letters and end up getting a $3.2\\%$ error rate instead of $3\\%$, then we should avoid using this new feature. Hence, we should try new things and get a numerical value for our error rate, then based on our result decide whether we want to keep the new feature or not. In addition, it is strongly recommended to do error analysis is on the cross validations rather than the test set. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Linear SVM (Spam Classification)\n",
      "\n",
      "(this may take 1 to 2 minutes) ...\n",
      "\n",
      "Train Accuracy:  99.85000000000001\n",
      "Expected accuracy (approx): 99.8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =========== Part 3: Train Linear SVM for Spam Classification ========\n",
    "#  In this section, you will train a linear classifier to determine if an\n",
    "#  email is Spam or Not-Spam.\n",
    "\n",
    "#Load the Spam Email dataset\n",
    "#You will have X, y in your environment\n",
    "train_data = sio.loadmat('spamTrain.mat')\n",
    "X = train_data['X']\n",
    "y = train_data['y']\n",
    "\n",
    "print('\\nTraining Linear SVM (Spam Classification)\\n')\n",
    "print('(this may take 1 to 2 minutes) ...\\n')\n",
    "\n",
    "C=0.1\n",
    "model = (svm.LinearSVC(C=C, loss='hinge', tol=0.001, max_iter=1000)).fit(X,y.ravel())\n",
    "p_train = model.predict(X)\n",
    "\n",
    "print('Train Accuracy: ', (np.mean(np.equal(p_train, y.T)))*100)\n",
    "print('Expected accuracy (approx): 99.8\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the trained Linear SVM on a test set ...\n",
      "\n",
      "Test Accuracy:  98.9\n",
      "Expected accuracy (approx): 98.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =================== Part 4: Test Spam Classification ================\n",
    "#  After training the classifier, we can evaluate it on a test set. We have\n",
    "#  included a test set in spamTest.mat\n",
    "\n",
    "#Load the test dataset\n",
    "#You will have Xtest, ytest in your environment\n",
    "test_data = sio.loadmat('spamTest.mat')\n",
    "Xtest = test_data['Xtest']\n",
    "ytest = test_data['ytest']\n",
    "\n",
    "print('\\nEvaluating the trained Linear SVM on a test set ...\\n')\n",
    "p_test = model.predict(Xtest)\n",
    "\n",
    "print('Test Accuracy: ', (np.mean(np.equal(p_test, ytest.T)))*100)\n",
    "print('Expected accuracy (approx): 98.5\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Top Predictors for Spam\n",
    "To better understand how the spam classiﬁer works, the parameters are inspected to see which words the classifier thinks are the most predictive of spam. The next step is to find the parameters with the largest positive values in the classiﬁer and displays the corresponding words (Figure 12). Thus, if an email contains words such as \"guarantee\", \"remove\", \"dollar\", and \"price\" (the top predictors shown in Figure 12), it is likely to be classiﬁed as spam.\n",
    "\n",
    "Image Source: ex6.pdf from Andrew Ng's Coursera course in Machine Learning\n",
    "![topspam.png](topspam.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top predictors of spam: \n",
      "\n",
      "vocab word  weights\n",
      "\n",
      "visit 87.19308359962946\n",
      "click 86.65099398452745\n",
      "guarante 74.28045564391603\n",
      "remov 60.806342183571836\n",
      "price 59.575379325485805\n",
      "dollar 59.51109928647399\n",
      "hour 54.86028766311198\n",
      "most 51.233909660779545\n",
      "credit 46.858530917173766\n",
      "you 42.662159218649386\n",
      "repli 41.86440208284136\n",
      "contact 40.447059133356184\n",
      "pleas 40.16580151991224\n",
      "nbsp 40.04630836026987\n",
      "pai 37.63121072291729\n"
     ]
    }
   ],
   "source": [
    "# ================= Part 5: Top Predictors of Spam ====================\n",
    "#  Since the model we are training is a linear SVM, we can inspect the\n",
    "#  weights learned by the model to understand better how it is determining\n",
    "#  whether an email is spam or not. The following code finds the words with\n",
    "#  the highest weights in the classifier. Informally, the classifier\n",
    "#  'thinks' that these words are the most likely indicators of spam.\n",
    "\n",
    "#Sort the weights and obtin the vocabulary list\n",
    "modelw = (model.coef_)*np.dot(y.T,X)\n",
    "weights = -np.sort(-modelw)\n",
    "idx = np.argsort(-modelw)\n",
    "vocabList = getVocabList()\n",
    "\n",
    "print('\\nTop predictors of spam: \\n')\n",
    "print('vocab word  weights\\n')\n",
    "for i in range(15):\n",
    "    print(vocabList[idx[0][i]], weights[0][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Optional (ungraded) exercise: Try your own emails\n",
    "In the starter code, two email examples (emailSample1.txt and emailSample2.txt) and two spam examples (spamSample1.txt and spamSample2.txt) are included. The last part of the exercise involves running the spam classfier over the ﬁrst spam example and classifies it using the learned SVM. Try the other examples provided and see if the classiﬁer gets them right. You can also try your own emails by replacing the examples (plain text ﬁles) with your own emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Processed Email ====\n",
      "\n",
      "\n",
      "\n",
      "Processed spamSample1.txt\n",
      "Spam Classification:  [1]\n",
      "(1 indicates spam, 0 indicates not spam)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =================== Part 6: Try Your Own Emails =====================\n",
    "#  Now that you've trained the spam classifier, you can use it on your own\n",
    "#  emails! In the starter code, we have included spamSample1.txt,\n",
    "#  spamSample2.txt, emailSample1.txt and emailSample2.txt as examples. \n",
    "#  The following code reads in one of these emails and then uses your \n",
    "#  learned SVM classifier to determine whether the email is Spam or \n",
    "#  Not Spam \n",
    "\n",
    "#Set the file to be read in (change this to spamSample2.txt,\n",
    "#emailSample1.txt or emailSample2.txt to see different predictions on\n",
    "#different emails types). Try your own emails as well!\n",
    "filename = 'spamSample1.txt'\n",
    "\n",
    "#Read and predict\n",
    "file_contents = open(filename)\n",
    "word_indices  = processEmail(file_contents)\n",
    "x             = emailFeatures(word_indices)\n",
    "\n",
    "p_spam = model.predict(x.T)\n",
    "\n",
    "print('\\nProcessed', filename)\n",
    "print('Spam Classification: ', p_spam)\n",
    "print('(1 indicates spam, 0 indicates not spam)\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Processed Email ====\n",
      "\n",
      "\n",
      "\n",
      "Processed spamSample2.txt\n",
      "Spam Classification:  [1]\n",
      "(1 indicates spam, 0 indicates not spam)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set the file to be read in (change this to spamSample2.txt,\n",
    "#emailSample1.txt or emailSample2.txt to see different predictions on\n",
    "#different emails types). Try your own emails as well!\n",
    "filename = 'spamSample2.txt'\n",
    "\n",
    "#Read and predict\n",
    "file_contents = open(filename)\n",
    "word_indices  = processEmail(file_contents)\n",
    "x             = emailFeatures(word_indices)\n",
    "\n",
    "p_spam = model.predict(x.T)\n",
    "\n",
    "print('\\nProcessed', filename)\n",
    "print('Spam Classification: ', p_spam)\n",
    "print('(1 indicates spam, 0 indicates not spam)\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Processed Email ====\n",
      "\n",
      "\n",
      "\n",
      "Processed emailSample1.txt\n",
      "Spam Classification:  [0]\n",
      "(1 indicates spam, 0 indicates not spam)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set the file to be read in (change this to spamSample2.txt,\n",
    "#emailSample1.txt or emailSample2.txt to see different predictions on\n",
    "#different emails types). Try your own emails as well!\n",
    "filename = 'emailSample1.txt'\n",
    "\n",
    "#Read and predict\n",
    "file_contents = open(filename)\n",
    "word_indices  = processEmail(file_contents)\n",
    "x             = emailFeatures(word_indices)\n",
    "\n",
    "p_spam = model.predict(x.T)\n",
    "\n",
    "print('\\nProcessed', filename)\n",
    "print('Spam Classification: ', p_spam)\n",
    "print('(1 indicates spam, 0 indicates not spam)\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Processed Email ====\n",
      "\n",
      "\n",
      "\n",
      "Processed emailSample2.txt\n",
      "Spam Classification:  [0]\n",
      "(1 indicates spam, 0 indicates not spam)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Set the file to be read in (change this to spamSample2.txt,\n",
    "#emailSample1.txt or emailSample2.txt to see different predictions on\n",
    "#different emails types). Try your own emails as well!\n",
    "filename = 'emailSample2.txt'\n",
    "\n",
    "#Read and predict\n",
    "file_contents = open(filename)\n",
    "word_indices  = processEmail(file_contents)\n",
    "x             = emailFeatures(word_indices)\n",
    "\n",
    "p_spam = model.predict(x.T)\n",
    "\n",
    "print('\\nProcessed', filename)\n",
    "print('Spam Classification: ', p_spam)\n",
    "print('(1 indicates spam, 0 indicates not spam)\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Optional (ungraded) exercise: Build your own dataset\n",
    "In this exercise, a preprocessed training set and test set was provided. These datasets were created using the same functions (processEmail and emailFeatures). For this optional (ungraded) exercise, build your own dataset using the original emails from the [SpamAssassin Public Corpus](http://spamassassin.apache.org/old/publiccorpus/).\n",
    "\n",
    "The task in this optional (ungraded) exercise is to download the original files from the public corpus and extract them. After extracting them, run the processEmail and emailFeatures functions on each email to extract a feature vector from each email to build a dataset $X, y$ of examples. Then randomly divide up the dataset into a training set, a cross validation set and a test set. Use processEmail to remove the email headers from the emails.\n",
    "\n",
    "When building your own dataset, try building your own vocabulary list by selecting the high frequency word that occur in the dataset and adding any additional features that you think might be useful. Try to use a highly optimized SVM toolboxes as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "machinelearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
